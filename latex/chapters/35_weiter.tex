\section{Stochastische Analysis und alternative Kursmodelle}

Dieses Kapitel soll einen Einblick in weiterführende mathematische Aspekte geben, die in dieser Arbeit nicht vertieft wurden.
Es ersetzt somit den obligatorischen Abschnitt \"Ausblick\" in einer Bachelorarbeit.

\paragraph{Motivation Stochastischer Differentialgleichungen}

Im Kapitel 5 wurde ein Aktienkurs durch die zeitdiskrete Übergangsgleichung
$$S_{n+1} = S_n \big(1 + \mu \Delta t + \sigma \sqrt{\Delta t}\,\varepsilon_{n+1}\big)$$
modelliert. Ziel des folgenden Abschnitts ist es, den Grenzübergang $\Delta t \to 0$ zu betrachten,
um eine kontinuierliche Beschreibung der Kursdynamik zu erhalten. Dabei soll die stochastische Differentialgleichung
$$dS_t = \mu S_t\,dt + \sigma S_t\,dW_t$$
hergeleitet werden, deren Lösung die geometrische Brownsche Bewegung ist. 
Anschaulich muss man sich nur noch von $\sqrt{\Delta t} \varepsilon_{n+1} \to dW_t$ überzeugen, wobei $W_t$ eine Brownsche Bewegung ist.
Sei $\Delta t = T/n$ und $(\varepsilon_k)_{k\ge 1}$ i.i.d. mit $\mathbb E[\varepsilon_k]=0$, $\mathbb Var(\varepsilon_k)=1$.
Definiere den skalierten Zufallsspaziergang
$$
W^{(n)}(t) := \sqrt{\Delta t}\sum_{k=1}^{\lfloor t/\Delta t\rfloor}\varepsilon_k,\qquad t\in[0,T].
$$
Dann gilt für jedes feste $t$ nach dem Zentralen Grenzwertsatz (ZGWS)
$$
W^{(n)}(t)\ \to\ \mathcal N(0,t).
$$
Und nach Kapitel 4 konvergiert $W^{(n)}$ sogar gegen eine Brownsche Bewegung $W$.

Insbesondere sind die diskreten Inkremente unabhängig und es gilt für $k=\lfloor t/\Delta t\rfloor$
$$
W^{(n)}(t+\Delta t)-W^{(n)}(t)\;=\;\sqrt{\Delta t}\,\varepsilon_{k+1}\ \sim\ \mathcal N(0,\Delta t).
$$
Mit $W^{(n)}\to W$ folgt damit für jedes $t$ die Verteilungskonvergenz der Inkremente
$$
\sqrt{\Delta t}\,\varepsilon_{k+1}
\;=\;W^{(n)}(t+\Delta t)-W^{(n)}(t)\ \to \ W_{t+\Delta t}-W_t,
$$
Durch Missbrauch der Notation erhält man
$$
\sqrt{\Delta t}\,\varepsilon_{n+1}\ \to\ dW_t.
$$
Insgesamt gilt
$$\Delta t \to dt, \qquad S_{t + \Delta t} - S_t \to dS_t$$
und $\sqrt{\Delta t} \varepsilon_{n+1} \to dW_t$
kann die Übergangsgleichung als stochastische Differentialgleichung interpretiert werden.
\qed

Formal werden stochastische Differentialgleichungen über das It\^o-Integral eingeführt, das wird 
im nächsten Abschnitt skizziert, für eine genaue Darstellung sei auf die Literatur verwiesen, z. B. Behrends \cite{behrends} oder Karatzas und Shreve \cite{karatzas_brownian_1991}.
Die Argumentation ist analog zu Kapitel 6 des Lehrbuchs von Behrends \cite{behrends}.

\subsection{Stochastische Differentialgleichungen}
TODO: weniger Lehrbuchmäßig formulieren, mehr auf Intuition und Zusammenhang mit dem bisherigen Text eingehen.
Ziel dieses Abschnitts ist es, die heuristische Schreibweise
$$
dS_t \;=\; a(S_t,t)\,dt \;+\; b(S_t,t)\,dW_t
$$
zu präzisieren: Man definiert das Integral gegen Brownsche Bewegung zunächst für \emph{elementare} (stückweise konstante, vorhersagbare) Prozesse, zeigt die It\^o-Isometrie, benutzt die Dichtheit dieser elementaren Prozesse und erhält so das It\^o-Integral für allgemeine quadratintegrierbare, vorhersagbare Integranden.

\paragraph{Grundaufbau}
Arbeitsraum ist ein filtrierter Wahrscheinlichkeitsraum
$$
(\Omega,\mathcal F,(\mathcal F_t)_{t\ge 0},\mathbb P).
$$
Eine Standard-\emph{Brownsche Bewegung} $W=(W_t)_{t\ge 0}$ ist ein $(\mathcal F_t)$-adaptierter Prozess mit $W_0=0$, stetigen Pfaden, unabhängigen Zuwächsen und
$$
W_t - W_s \sim \mathcal N(0,t-s)\quad\text{für }0\le s<t.
$$
Ein Prozess $X=(X_t)_{t\ge 0}$ heißt \emph{adaptiert}, falls er bezüglich der vorhersagbaren $\sigma$-Algebra messbar ist; für die Konstruktion genügt es, zuerst stückweise konstante, adaptierte Prozesse zu betrachten.

\paragraph{Elementare (adaptierte) Prozesse und ihr Integral}
Fixiere $T>0$. Ein Prozess $H$ heißt \emph{elementar adaptiert} auf $[0,T]$, wenn es eine Zerlegung $0=t_0<t_1<\dots<t_n=T$ und Zufallsvariablen $\xi_i\in L^2(\Omega,\mathcal F_{t_i},\mathbb P)$, $i=0,\dots,n-1$, gibt mit
$$
H_t \;=\; \sum_{i=0}^{n-1} \xi_i\,\mathbf 1_{(t_i,t_{i+1}]}(t),\qquad t\in[0,T].
$$
Für solche $H$ definiert das \emph{It\^o-Integral} gegen $W$ durch
$$
\int_0^T H_s\,dW_s \;:=\; \sum_{i=0}^{n-1} \xi_i\,\big(W_{t_{i+1}}-W_{t_i}\big).
$$
Zentrale Eigenschaften (für $H$ wie oben):
$$
\mathbb E\!\left[\int_0^T H_s\,dW_s\right] \;=\; 0,\qquad
\mathbb E\!\left[\Big(\int_0^T H_s\,dW_s\Big)^{\!2}\right] \;=\; \mathbb E\!\left[\int_0^T H_s^2\,ds\right]
$$
(dies ist die \emph{It\^o-Isometrie}). Für die zeitabhängige Version definiert man für $t\in[0,T]$
$$
\int_0^t H_s\,dW_s \;:=\; \sum_{i=0}^{n-1} \xi_i\,\big(W_{t\wedge t_{i+1}}-W_{t\wedge t_i}\big),
$$
und erhält einen $(\mathcal F_t)$-Martingalprozess mit Quadratischer Variation
$[\!\int_0^\cdot H_s dW_s]_t = \int_0^t H_s^2\,ds$.

\paragraph{Dichtheit der elementaren Prozesse}
Sei $L^2_{\mathrm{pred}}(\Omega\times[0,T])$ der Raum aller vorhersagbaren Prozesse $X$ mit
$\|X\|_{2,T}^2 := \mathbb E\!\int_0^T |X_s|^2 ds <\infty$.
Dann ist die Klasse der elementaren vorhersagbaren Prozesse dicht in $L^2_{\mathrm{pred}}(\Omega\times[0,T])$ bezüglich der Norm $\|\cdot\|_{2,T}$. (Siehe z. B. Karatzas und Shreve \cite{karatzas_brownian_1991}, S. 132f.)
\paragraph{Definition des It\^o-Integrals}
Für einen allgemeinen vorhersagbaren Prozess $X\in L^2_{\mathrm{pred}}(\Omega\times[0,T])$ wähle eine Folge elementarer Prozesse $H^{(n)}$ mit
$\|H^{(n)}-X\|_{2,T}\to 0$. Dann definieren wir
$$
\int_0^T X_s\,dW_s \;:=\; L^2\text{-}\lim_{n\to\infty}\;\int_0^T H^{(n)}_s\,dW_s,
$$
wobei der Limes aufgrund der It\^o-Isometrie existiert und nicht von der approximierenden Folge abhängt. Für jedes $t\in[0,T]$ definiert man analog den Prozess
$$
\Big(\int_0^t X_s\,dW_s\Big)_{t\in[0,T]},
$$
und es gilt weiterhin die It\^o-Isometrie
$$
\mathbb E\!\left[\Big(\int_0^T X_s\,dW_s\Big)^{\!2}\right] \;=\; \mathbb E\!\left[\int_0^T X_s^{2}\,ds\right],
$$
sowie die Martingaleigenschaft von $M_t:=\int_0^t X_s\,dW_s$. Für lokal quadratintegrierbare $X$ erhält man das Integral durch Lokalisierung via Stoppzeiten.

\paragraph{Verbindung zur heuristischen Notation}
Mit dieser Konstruktion ist die Schreibweise
$$
dS_t \;=\; a(S_t,t)\,dt \;+\; b(S_t,t)\,dW_t
$$
präzise zu lesen als Integralgleichung
$$
S_t \;=\; S_0 \;+\; \int_0^t a(S_s,s)\,ds \;+\; \int_0^t b(S_s,s)\,dW_s,
$$
wobei $b(S_\cdot,\cdot)$ vorhersagbar und quadratintegrierbar sein muss.


\subsection{Charakterisierung alternativer Kursmodelle durch stochastische Differentialgleichungen}

Die GBM nimmt konstante Volatilität und lognormale Renditen an; sie erzeugt weder Sprünge noch Volatilitäts-Clustering oder Smile/Skew.
Glasserman \cite{glasserman2003monte} beschreibt verschiedene Ansätze zur Modellierung dieser Phänomene:

\paragraph{Lokale Volatilität}
Deterministisch zeit- und zustandsabhängige Volatilität:
$$
dS_t \;=\; \mu(t,S_t)\,S_t\,dt \;+\; \sigma_{\mathrm{loc}}(t,S_t)\,S_t\,dW_t.
$$
Spezialfall CEV:
$$
dS_t \;=\; \mu S_t\,dt \;+\; \sigma\,S_t^{\beta}\,dW_t,\qquad \beta\in\mathbb R,
$$
wodurch die Volatilität bei kleinen/größeren Preisen relativ ansteigt/abnimmt.

\paragraph{Stochastische Volatilität}
Volatilität ist selbst ein Zufallsprozess.
Heston:
$$
\begin{aligned}
dS_t &= \mu S_t\,dt + \sqrt{V_t}\,S_t\,dW_t^{(1)},\\
dV_t &= \kappa(\theta - V_t)\,dt + \xi\sqrt{V_t}\,dW_t^{(2)},\quad d \langle W^{(1)},W^{(2)}\rangle_t=\rho\,dt,
\end{aligned}
$$
mit Feller-Bedingung $2\kappa\theta\ge \xi^2$ für Positivität von $V_t$.

\paragraph{Sprung-Diffusions-Modelle}
Diffusion mit seltenen Sprüngen modelliert durch Poisson-Prozesse.
Merton:
$$
\frac{dS_t}{S_t} \;=\; \big(\mu - \lambda \kappa_J\big)\,dt \;+\; \sigma\,dW_t \;+\; (J_t-1)\,dN_t,
$$
wobei $N_t$ Poisson mit Intensität $\lambda$ ist, $J_t$ die Sprunggröße (z.\,B. lognormal) und $\kappa_J=\mathbb E[J_t-1]$; der Driftterm kompensiert die Sprünge.

\paragraph{Regimewechsel-Modelle}
Parameter schalten gemäß einer (verborgenen) Markov-Kette $X_t$:
$$
dS_t \;=\; \mu_{X_t} S_t\,dt \;+\; \sigma_{X_t} S_t\,dW_t.
$$
Erfasst Phasen wie Krisen und ruhige Märkte.

\subsection{Simulation stochastischer Differentialgleichungen}
Zu einer SDE
$$
dX_t \;=\; a(X_t,t)\,dt \;+\; b(X_t,t)\,dW_t,\quad t\in[0,T],
$$
wird ein Zeitschrittgitter $t_n=n\Delta t$ eingeführt und der kontinuierliche Prozess durch eine zeitdiskrete Approximation ersetzt. Erwartungswerte werden via Monte‑Carlo über viele simulierte Pfade geschätzt. Der Gesamtfehler setzt sich aus Diskretisierungsfehler (Bias) und Monte‑Carlo‑Fehler $O(M^{-1/2})$ zusammen.

Die Differentialgleichung wird mit Euler (deterministisch) und Euler–Maruyama (stochastisch) diskretisiert (vgl. Bärwolff \cite{Baerwolff2025} u. a. S. 269, 466f.).
Für die ODE $\dot x(t)=a(x(t),t)$ liefert das explizite Euler‑Schema
$$
x_{n+1} \;=\; x_n \;+\; a(x_n,t_n)\,\Delta t.
$$
Ersetzt man nun formal $dW_t$ durch $\Delta W_n:=W_{t_{n+1}}-W_{t_n}\sim \mathcal N(0,\Delta t)$, erhält man die natürliche stochastische Erweiterung, das Euler–Maruyama‑Schema:
$$
X_{n+1} \;=\; X_n \;+\; a(X_n,t_n)\,\Delta t \;+\; b(X_n,t_n)\,\Delta W_n
\;=\; X_n \;+\; a(X_n,t_n)\,\Delta t \;+\; b(X_n,t_n)\,\sqrt{\Delta t}\,\varepsilon_{n+1},
$$
mit i.i.d. $\varepsilon_{n}\sim\mathcal N(0,1)$. 
Ähnlich wurde im Hauptteil die geometrische Brownsche Bewegung hergeleitet.
Es folgt ein R-Programm zur Simulation eines CEV-Modells (s. u.).

\begin{lstlisting}
simulate_cev_paths <- function(S0, mu, sigma, beta, dt, nsteps, npaths) {
  S <- matrix(NA, nrow = nsteps + 1, ncol = npaths)
  S[1, ] <- S0
  for (i in 1:nsteps) {
    Z <- rnorm(npaths)
    S[i+1, ] <- S[i, ] + mu * S[i, ] * dt + sigma * (S[i, ]^beta) * sqrt(dt) * Z
    S[i+1, ][S[i+1, ] <= 0] <- 1e-8
  }
  return(S)
}
\end{lstlisting}

\subsection{Fallstudie: CEV (Constant Elasticity of Variance) Modell}
Das CEV-Modell ist ein Spezialfall der lokalen Volatilität mit
$$
dS_t \;=\; \mu\,S_t\,dt \;+\; \sigma\,S_t^{\beta}\,dW_t,\qquad \beta\in\mathbb R.
$$
\begin{itemize}
\item $\beta=1$ ergibt GBM. 
\item $\beta<1$ erhöht die relative Volatilität bei kleinen Preisen (dickere linke Schwänze).
\item $\beta>1$ verstärkt Schwankungen bei hohen Preisen.
\end{itemize}

Mit diskreten Beobachtungen $S_{t_i}$ im Abstand $\Delta t$ liefert das Euler‑Schema
$$
\Delta S_i \approx \mu S_{t_i}\Delta t + \sigma S_{t_i}^{\beta}\sqrt{\Delta t}\,\varepsilon_i,\quad \varepsilon_i\sim\mathcal N(0,1).
$$
Die nächste Herausforderung ist die Kalibrierung (Parameterschätzung) aus gegebenen Daten. Die Arbeit 
orientiert sich im folgenden an Höök und Lindström \cite{hook2015estimation} (2015). Eine eigene Implementierung in
R wird danach mit dem Softwarepaket Sim.DiffProc. \cite{iooss2006sde} (2020) verglichen, das auf Monte-Carlo-Simulationen basiert.

\paragraph{Exkurs: (Quasi) Maximum-Likelihood-Schätzung (MLE, QMLE)}
Ziel ist die Parameterschätzung aus diskreten Beobachtungen $S_{t_0},\dots,S_{t_n}$ eines SDE
$dS_t=a(S_t,t)\,dt+b(S_t,t)\,dW_t$. Grundidee: Wähle Parameter $\theta$ so, dass die beobachteten Daten unter dem Modell am wahrscheinlichsten sind. Für diskrete Beobachtungen $S_{t_0},\dots,S_{t_n}$ eines (Markov‑)Modells gilt
$$
L(\theta) \;=\; \prod_{i=0}^{n-1} p_\theta\!\big(S_{t_{i+1}},\,\Delta t \,\big|\, S_{t_i}\big), 
\qquad
\ell(\theta)=\log L(\theta)=\sum_{i=0}^{n-1}\log p_\theta\!\big(S_{t_{i+1}},\Delta t \,\big|\, S_{t_i}\big),
$$
wobei $p_\theta(\,\cdot\,|\,\cdot)$ die bedingte Übergangsdichte in Schrittweite $\Delta t$ ist. Der MLE ist
$$
\widehat\theta\;=\;\arg\max_{\theta}\ \ell(\theta).
$$
Das Vorgehen gliedert sich in drei Schritte. Falls $p_\theta$ nicht explizit bekannt ist, wird eine Quasi-Likelihood (QMLE) verwendet, zum Beispiel via Euler-Maruyama-Approximation.
$$
S_{t_{i+1}}-S_{t_i}\mid S_{t_i}\ \approx\ \mathcal N\!\big(a_\theta(S_{t_i},t_i)\,\Delta t,\; b_\theta^2(S_{t_i},t_i)\,\Delta t\big),
$$
Dann wird die (Pseudo‑)Log‑Likelihood aufgestellt, und numerisch maximiert.

\paragraph{Herleitung des Quasi‑MLE (Euler‑Pseudo‑Likelihood).}
Bei uns ist $\theta$ der Parametervektor $(\mu,\sigma,\beta)$. Der QMLE basiert hier auf der lokalen Gauß‑Approximation der Übergangsdichte via Euler‑Maruyama. Allgemein für
$dX_t=a(X_t,t)\,dt+b(X_t,t)\,dW_t$ gilt für kleine $\Delta t$ und bedingt auf $X_{t_i}$:
$$
\Delta X_i \mid X_{t_i}\ \approx\ \mathcal N\!\big(a(X_{t_i},t_i)\,\Delta t,\; b^2(X_{t_i},t_i)\,\Delta t\big).
$$
Damit ergibt sich die (Pseudo‑)Log‑Likelihood
$$
\ell \;=\; -\tfrac12\sum_i \Big[
\log\!\big(2\pi\,b^2(X_{t_i},t_i)\Delta t\big)
+\frac{\big(\Delta X_i-a(X_{t_i},t_i)\Delta t\big)^2}{b^2(X_{t_i},t_i)\Delta t}
\Big].
$$
Für CEV ($a(s,t)=\mu s$, $b(s,t)=\sigma s^{\beta}$) wird das zu
$$
\ell(\mu,\sigma,\beta)
= -\tfrac12\sum_i\Big[
\log\!\big(2\pi\,\sigma^2 S_{t_i}^{2\beta}\Delta t\big)
+\frac{\big(\Delta S_i-\mu S_{t_i}\Delta t\big)^2}{\sigma^2 S_{t_i}^{2\beta}\Delta t}
\Big].
$$
Maximiert man zunächst nach $\mu$ und $\sigma^2$ (für gegebenes $\beta$), erhält man geschlossene Formen:
$$
\widehat{\mu}(\beta)
= \frac{\sum_i \Delta S_i\, S_{t_i}^{\,1-2\beta}}{\sum_i S_{t_i}^{\,2-2\beta}\,\Delta t},
\qquad
\widehat{\sigma}^{\,2}(\beta)
= \frac{1}{n}\sum_i \frac{\big(\Delta S_i-\widehat{\mu}(\beta)\,S_{t_i}\Delta t\big)^2}{S_{t_i}^{\,2\beta}\,\Delta t}.
$$
Setze diese in $\ell$ ein (Profil‑Likelihood) und optimiere numerisch über $\beta$.
Hinweis: Für $\beta=1$ (GBM) fallen die Formeln auf die bekannten MLE‑Schätzer für Drift und Volatilität zurück.

\paragraph{Implementierung in R mit dem Paket Sim.Diffproc}

\begin{lstlisting}
S_ts <- ts(dax$Price, deltat = dt_daily)

drift     <- expression(theta[1] * x)           # theta1 = mu
diffusion <- expression(theta[2] * x^theta[3])    # theta2 = sigma, theta3 = beta

fit_cev <- fitsde(
  data      = S_ts,
  drift     = drift,
  diffusion = diffusion,
  start     = start_vals,
  pmle      = "euler",
  optim.method = "L-BFGS-B",
  lower     = c(mu = -Inf, sigma = 1e-8, beta = 0.0),
  upper     = c(mu =  Inf, sigma =  Inf, beta = 3.0)
)
\end{lstlisting}

Es folgt ein Backtest mit den geschätzten Parametern (vgl. Kapitel 7).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/cev_dax_backtest.png}
    \caption{Backtest CEV-Modell für den DAX}
    \label{fig:cev_dax_backtest}
\end{figure}

Die Überdeckungswahrscheinlichkeit liegt bei 82\%. Dieselbe Konfiguration
lieferte für die gemetrische Brownsche Bewegung eine Überdeckungswahrscheinlichkeit von 86\%.
Ein feinerer Vergleich der Kalibrierungen folgt im nächsten Abschnitt. 

\paragraph{Eigene Implementierung in R mit der Euler‑Pseudo‑Likelihood}
Zur numerischen Optimierung wird das Paket nloptr \cite{nloptr} verwendet.

\begin{lstlisting}
estimate_cev <- function(S, dt = 1/252) {
  S <- as.numeric(S)
  stopifnot(is.numeric(S), all(is.finite(S)), length(S) >= 3)
  
  dS  <- diff(S)
  S0  <- S[-length(S)]
  eps <- 1e-12
  S0[S0 <= 0] <- eps
  sigma_start <- sd(dax_in$logret)  / sqrt(dt_daily)
  mu_start    <- mean(dax_in$logret) / dt_daily - sigma_start / 2
  beta_start  <- 1.0
  init <- c(mu_start, sigma_start, beta_start) 
  
  negloglik <- function(par) {
    mu    <- par[1]
    sigma <- par[2]
    beta  <- par[3]
    if (!is.finite(mu) || !is.finite(sigma) || !is.finite(beta)) return(1e12)
    if (sigma <= 0 || beta <= 0 || beta >= 5) return(1e12)
    
    denom <- (sigma*sigma) * (S0^(2*beta)) * dt
    denom[denom <= 0] <- eps
    val <- 0.5 * sum(log(2*pi*denom) + ((dS - mu*S0*dt)^2)/denom)
    if (!is.finite(val)) val <- 1e12
    as.numeric(val)
  }
  
  res <- nloptr(
    x0     = init,
    eval_f = negloglik,
    lb     = c(-Inf, 1e-8, 1e-6),
    ub     = c( Inf,  Inf,  4.999),
    opts   = list(algorithm = "NLOPT_LN_SBPLX", xtol_rel = 1e-8, maxeval = 3000)
  )
  
  list(mu = res$solution[1], sigma = res$solution[2], beta = res$solution[3])
}
\end{lstlisting}

\subsection{Ergebnisse und Vergleich der Kalibrierungen}

Die beiden Kalibrierungen werden auf die Aktienkursdaten des DAX, Lufthansa und Adesso angewendet. Zusätzlich werden
Backtests mit den geschätzten Parametern durchgeführt. Als Vergleich wird die GBM ($\beta=1$) verwendet.
