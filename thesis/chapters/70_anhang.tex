\section{Notation und Ergebnisse aus der Stochastik}
\subsection{Notation}
\begin{itemize}
    \item $E(X)$ bzw. $E[X]$ - Erwartungswert
    \item $V(X)$ bzw. $V[X]$ - Varianz
    \item $X \sim \mathcal N(\mu, \sigma^2)$ - normalverteilte Zufallsvariable mit Erwartungswert $\mu$ und Varianz $\sigma^2$
    \item $(\Omega, \mathcal F, P)$ - Wahrscheinlichkeitsraum
    \item $W_t$ - Brownsche Bewegung (Wiener-Prozess)
    \item $\mathbf 1_A(\omega)$ - nimmt $1$ an, falls $\omega \in A$, sonst $0$. Mit $\mathbf 1_{\{f \gt 0\}}$ ist $1_A$ mit $A=f^{-1}([0, \infty))$ gemeint
    \item $O(x)$ - beliebiger Term, der sich (im Limes) linear zu $x$ verhält bzw. Menge der Funktionen die sich so verhalten.
$$\lim_{x \to x_0} \left \vert \frac{f(x)}{g(x)} \right \vert  \lt M \iff f \in O(g(x))$$    
    \item $o(x)$ - beliebiger Term, der (im Limes) schneller als $x$ verschwindet
$$\lim_{x \to x_0} \frac{f(x)}{g(x)} = 0 \iff f \in o(g(x))$$    
    \item $L^2(\Omega)$ - Der normierte Vektorraum der Zufallsvariablen $X : \Omega \to \Bbb R$, wobei $E(X^2) \lt \infty$ ist.
    \item i.i.d. - Die Zufallsvariablen sind stochastisch Unabhängig und identisch verteilt.
    \item GBM - geometrische Brownsche Bewegung
    \item CEV - Constant Elasticity of Variance (Modell)
    \item MSE - Mean Squared Error
    \item NRMSE - Normalized Root Mean Squared Error
    \item RMSE - Root Mean Squared Error
    \item MAPE - Mean Absolute Percentage Error
\end{itemize}

\subsection{Konvergenzbegriffe}

\begin{itemize}
    \item $X_n \xrightarrow{\mathrm{pktw.}} X$ - punktweise Konvergenz (Für jedes $\omega \in \Omega$ gilt: $X_n(\omega) \to X(\omega)$)
    \item $X_n \xrightarrow{\mathrm{glm.}} X$ - gleichmäßige Konvergenz (Für jede $\varepsilon > 0$ gilt: $\sup_{\omega \in \Omega} |X_n(\omega) - X(\omega)| < \varepsilon$ für $n$ groß genug)
    \item $X_n \xrightarrow{\mathrm{d}} X$ - Konvergenz in Verteilung (Die Verteilungsfunktion $F_{X_n}$ von $X_n$ konvergiert punktweise gegen die Verteilungsfunktion $F_X$ von $X$, mindestens an den Stetigkeitsstellen von $F_X$)
    \item $X_n \xrightarrow{\mathrm{p}} X$ - Konvergenz in Wahrscheinlichkeit (Für jede $\varepsilon > 0$ gilt: $P(|X_n - X| > \varepsilon) \to 0$)
    \item $X_n \xrightarrow{\mathrm{f.s.}} X$ - fast sichere Konvergenz (Für fast alle $\omega \in \Omega$ gilt: $X_n(\omega) \to X(\omega)$)
\end{itemize}

Auf das Verhältnis zwischen den Konvergenzbegriffen wird bei Bedarf eingegangen.

\subsection{Ergebnisse aus der Stochastik}

\begin{satz}[Cramér-Wold-Technik, Henze 2023 \cite{henze} S. 225]
Seien $(X_n)_{n \in \Bbb N}$ und $X$ Zufallsvektoren in $\Bbb R^d$. Dann sind folgende Aussagen äquivalent:
\begin{enumerate}
    \item $X_n \xrightarrow{d} X$ für $n \to \infty$.
    \item Für alle $t \in \Bbb R^d$ gilt $t^T X_n \xrightarrow{d} t^T X$ für $n \to \infty$.
\end{enumerate}
Wird hier nicht bewiesen. \qed \\
Die Cramér-Wold-Technik erlaubt es, mehrdimensionale Verteilungskonvergenz auf eindimensionale Berechnungen zurückzuführen.
\end{satz}

\begin{satz}[Zentraler Grenzwertsatz von Lindeberg-Feller, Henze 2023 \cite{henze} S. 223]
Die Zufallsvariablen $X_{ni}, 1 \le i \le k_n \to \infty$ seien für jedes $n \in \Bbb N$ stochastisch unabhängig mit
$E(X_{ni})=0$ und $\sigma^2_{ni} = \text{Var}(X_{ni}) \lt \infty$.
Setze $s_n^2 = \sum_{i=1}^{k_i} \sigma_{ni}^2$. Gilt die Lindeberg-Bedingung
$$(L) \quad \lim_{n \to \infty} L_n(\varepsilon) = \frac{1}{s_n^2} \sum_{i=1}^{k_n} E(X^2_{ni} 1_{\{\vert X_{ni} \vert \gt \varepsilon s_n\}})=0, \quad \forall \varepsilon \gt 0$$
Dann folgt
$$\frac{1}{s_n} \sum_{i=1}^{k_n} X_{ni} \underset{n \to \infty}{\overset d \longrightarrow} \mathcal N(0,1).$$
Wird hier nicht bewiesen. \qed
\end{satz}

\begin{lemma}[Reihenkriterium für fast sichere Konvergenz, Henze 2023 \cite{henze} S. 201]
Sei $(X_n)_{n \in \Bbb N}$ eine Folge von Zufallsvariablen. Wenn es eine Reihe $\sum_{n=1}^\infty a_n \lt \infty$ mit $a_n \geq 0$ gibt, so dass
$$P(|X_n| > \varepsilon) \leq a_n \quad \text{für alle } n \in \Bbb N \text{ und jedes } \varepsilon > 0,$$
dann konvergiert $X_n$ fast sicher gegen $0$, d.h.
$$P\left(\lim_{n \to \infty} X_n = 0\right) = 1.$$
Wird hier nicht bewiesen. \qed
\end{lemma}

\begin{satz}[Satz von Pratt, Elstrod \cite{elstrod} S. 280, vereinfacht]
Dieser technische Satz erlaubt es, den Grenzübergang und den Erwartungswert zu vertauschen, und
kommt im Folgenden bei der Herleitung der Black-Scholes-Formel zum Einsatz.
Sei $(X_n) \longrightarrow X$ eine Folge von Zufallsvariablen, die fast-überall konvergiert,
und $(Y_n) \longrightarrow Y$, $(Z_n) \longrightarrow Z$ ebenfalls.
Gilt (1) $Y_n \le X_n \le Z_n$ und (2) $E(Y_n) \longrightarrow E(Y)$, $E(Z_n) \longrightarrow E(Z)$,
dann folgt $E(X_n) \longrightarrow E(X)$. Wird hier nicht bewiesen. \qed
\end{satz}

\begin{defprop}[Monte-Carlo-Verfahren]
Monte-Carlo-Verfahren sind stochastische Simulationsmethoden, die zur numerischen
Lösung von Problemen verwendet werden, insbesondere zur Berechnung von Integralen
und Erwartungswerten. Sie basieren auf der Erzeugung von Zufallszahlen und
der statistischen Analyse der Ergebnisse. Man verwendet den \textit{Monte-Carlo-Schätzer}
$$
\hat{I}_n = \frac{1}{n} \sum_{i=1}^n f(X_i),
$$
wobei $X_1, X_2, \ldots, X_n$ unabhängige und identisch verteilte Zufallsvariablen
sind, die der Verteilung von $X$ folgen. Der Schätzer konvergiert fast sicher
gegen den wahren Erwartungswert $I = E[f(X)]$, wenn $n \to \infty$.
\textit{Beweis.} Nach dem Gesetz der großen Zahlen gilt
$$\hat{I}_n \longrightarrow E[f(X)] = I, \quad \text{fast sicher.}$$ \qed
\end{defprop}
\newpage
\section{Vergleich von geometrischer Brownscher Bewegung und Constant Elasticity of Variance Modell}

\begin{center}
    \begin{sideways}
        \resizebox{0.8\textheight}{!}{
            \csvautotabular{../r/results/compare_models.csv}
        }
    \end{sideways}
    \captionof{table}{Vergleich der Modelle GBM und CEV über verschiedene Backtests und Metriken: 
    Hitratio - größer ist besser; RMSE - kleiner ist besser; 
    MAPE - kleiner ist besser; NRMSE - kleiner ist besser} 
    \label{fig:table_gbm_cev}
\end{center}
